# user_data/TFT_PPO_Training/scripts/optuna_tuning.py
from __future__ import annotations

import math
import inspect
import pathlib
from typing import Callable, Dict, Any

import numpy as np
import optuna
import yaml
import gymnasium as gym

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

from TFT_PPO_modules.performance_metrics import performance_metrics  # (ë‚¨ê²¨ë‘ : í•„ìš”ì‹œ ì‚¬ìš©)
from TFT_PPO_modules.trading_env import TradingEnv                  # (íƒ€ì… íŒíŠ¸ ìš©ë„)
from TFT_PPO_Training.scripts.utils import set_seed

# =========================
# TradingEnv ì†ŒìŠ¤ ê²€ì¦ ê°•í™” (pathlib ê¸°ë°˜)
# =========================
src = pathlib.Path(inspect.getsourcefile(TradingEnv)).resolve()
print(f"[ENV-CHECK] TradingEnv from: {src}")

# optuna_tuning.py ìœ„ì¹˜ ê¸°ì¤€ ê¸°ëŒ€ ê²½ë¡œ (CWD ë¬´ê´€)
base = pathlib.Path(__file__).resolve().parents[2]  # .../user_data
expected = (base / "TFT_PPO_modules" / "trading_env.py").resolve()
if src != expected:
    raise RuntimeError(f"Loaded WRONG TradingEnv: {src} (expected: {expected})")


# =========================
# Sampler / Pruner builders
# =========================
def _build_sampler(scfg: Dict[str, Any]):
    t = scfg.get("type", "TPESampler")
    if t == "TPESampler":
        return optuna.samplers.TPESampler(seed=scfg.get("seed", 42))
    return optuna.samplers.TPESampler(seed=42)


def _build_pruner(pcfg: Dict[str, Any]):
    t = pcfg.get("type", "MedianPruner")
    if t == "MedianPruner":
        return optuna.pruners.MedianPruner(
            n_startup_trials=pcfg.get("n_startup_trials", 5),
            n_warmup_steps=pcfg.get("n_warmup_steps", 2),
        )
    elif t == "SuccessiveHalvingPruner":
        return optuna.pruners.SuccessiveHalvingPruner(
            min_resource=pcfg.get("min_resource", 1),
            reduction_factor=pcfg.get("reduction_factor", 3),
        )
    return optuna.pruners.MedianPruner()


# =========================
# Small utilities
# =========================
def _as_float(x):
    """ë¬¸ìì—´ì´ë‚˜ ë‹¤ë¥¸ íƒ€ì…ì„ floatë¡œ ë³€í™˜í•˜ê³  ìœ íš¨ì„± ê²€ì¦"""
    try:
        return float(x)
    except Exception:
        raise ValueError(f"search_space value must be float-compatible. got={x} ({type(x)})")


def _suggest_params(trial: optuna.trial.Trial, space: Dict[str, Any]) -> Dict[str, Any]:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ í•¨ìˆ˜ - ë¶„í¬ ì•ˆì „ì¥ì¹˜ í¬í•¨"""
    p = {}
    for k, spec in space.items():
        t = spec["type"]
        if t in ("loguniform", "uniform"):
            low = _as_float(spec["low"])
            high = _as_float(spec["high"])
            if low > high:
                low, high = high, low
            if t == "loguniform":
                if low <= 0:
                    raise ValueError(f"{k}: loguniform low must be > 0 (got {low})")
                p[k] = trial.suggest_float(k, low, high, log=True)
            else:
                p[k] = trial.suggest_float(k, low, high)
        elif t == "categorical":
            p[k] = trial.suggest_categorical(k, spec["choices"])
        else:
            raise ValueError(f"Unknown search type: {t}")
    return p


def make_env_with_offset(df, tft, features, offset, **kwargs):
    """ì„œë¡œ ë‹¤ë¥¸ ì‹œì‘ ì˜¤í”„ì…‹ì„ ê°€ì§„ í™˜ê²½ ìƒì„±"""
    sub = df.iloc[offset:].reset_index(drop=True)
    return lambda: TradingEnv(sub, tft, features, **kwargs)


def rolling_tft_encode(tft_model, X, win=96):
    """
    TFT ëª¨ë¸ë¡œ ë¡¤ë§ ì„ë² ë”© ê³„ì‚° (ìºì‹œìš©)
    X: (N, F) float32 (fp.features ìˆœì„œ)
    ë°˜í™˜: (N, d_model) enc_lastë¥¼ ë¡¤ë§ìœ¼ë¡œ ìƒì„±. ë¶€ì¡±í•œ ì•êµ¬ê°„ì€ 0.
    """
    import torch

    tft_model.eval()
    d_model = tft_model.hidden_size
    embs = np.zeros((len(X), d_model), dtype=np.float32)

    with torch.no_grad():
        for i in range(win, len(X)):
            window = X[i - win : i]
            window_tensor = torch.FloatTensor(window).unsqueeze(0).to(tft_model.device)  # (1, win, F)
            try:
                if hasattr(tft_model.tft, "encoder"):
                    encoder_output = tft_model.tft.encoder({"encoder_cont": window_tensor})  # (1, win, d_model)
                    enc_last = encoder_output[:, -1, :].squeeze(0).cpu().numpy()
                    embs[i] = enc_last
                else:
                    tft_output = tft_model.tft({"encoder_cont": window_tensor})
                    if hasattr(tft_output, "encoder_output"):
                        enc_last = tft_output.encoder_output[:, -1, :].squeeze(0).cpu().numpy()
                        embs[i] = enc_last
                    else:
                        raise Exception("No encoder output found")
            except Exception:
                # Fallback: ê°„ë‹¨í•œ ì„ í˜• íˆ¬ì˜
                if not hasattr(tft_model, "_cache_proj"):
                    tft_model._cache_proj = torch.nn.Linear(window.shape[1], d_model).to(tft_model.device)
                proj_in = torch.FloatTensor(window.mean(axis=0)).unsqueeze(0).to(tft_model.device)
                embs[i] = tft_model._cache_proj(proj_in).squeeze(0).cpu().numpy()

    print(f"[TFT Cache] Generated embeddings: {embs.shape}, mean={embs.mean():.6f}, std={embs.std():.6f}")
    return embs


def _extract_trade(infos, as_delta=False, _state={"last": 0}):
    """VecEnv/ë˜í¼ì—ì„œ trade ê°’ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ (per-step í”Œë˜ê·¸ ëŒ€ì‘)"""
    v = 0
    if isinstance(infos, (list, tuple)):
        # DummyVecEnv: ì²« ë²ˆì§¸ infoë§Œ í™•ì¸ (per-step í”Œë˜ê·¸)
        for info in infos:
            if isinstance(info, dict):
                # ì—¬ëŸ¬ í‚¤ëª… fallback ì§€ì›
                if "trade" in info:
                    v = int(info["trade"])
                    break
                elif "did_trade" in info:
                    v = int(info["did_trade"])
                    break
                elif "trade_flag" in info:
                    v = int(info["trade_flag"])
                    break
    elif isinstance(infos, dict):
        # ì—¬ëŸ¬ í‚¤ëª… fallback ì§€ì›
        if "trade" in infos:
            v = int(infos["trade"])
        elif "did_trade" in infos:
            v = int(infos["did_trade"])
        elif "trade_flag" in infos:
            v = int(infos["trade_flag"])
    
    if not as_delta:
        return v
    
    # delta ëª¨ë“œ: ì´ì „ ê°’ê³¼ì˜ ì°¨ì´ ë°˜í™˜ (ëˆ„ì  ì¹´ìš´í„° ëŒ€ì‘)
    delta = max(0, v - _state["last"])
    _state["last"] = v
    return delta


def _as_scalar(x):
    """ë°°ì—´ì„ ìŠ¤ì¹¼ë¼ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    arr = np.array(x)
    return float(arr.reshape(-1)[0])


def _as_bool(x):
    """ë°°ì—´ì„ boolë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    arr = np.array(x)
    return bool(arr.reshape(-1)[0])


def _audit_env(ev):
    """í‰ê°€ í™˜ê²½ ì •ìƒ ì‘ë™ í™•ì¸ (VecEnv API ì•ˆì „ ì²˜ë¦¬)"""
    out = ev.reset()
    obs = out[0] if isinstance(out, tuple) else out
    
    acts = [1, 2] * 10  # long/short ë²ˆê°ˆì•„
    pos_rewards = 0
    trades = 0
    
    for a in acts:
        a_in = [a] if hasattr(ev, "num_envs") else a
        ret = ev.step(a_in)
        
        # Gymnasium API: (obs, reward, terminated, truncated, infos)
        if len(ret) == 5:
            obs, r, term, trunc, infos = ret
            done = _as_bool(term) or _as_bool(trunc)
        else:
            obs, r, done, infos = ret  # êµ¬ë²„ì „ í˜¸í™˜
            done = _as_bool(done)
        
        pos_rewards += int(_as_scalar(r) > 0)
        trades += _extract_trade(infos)
        
        if done:
            break
    
    print(f"[AUDIT] pos_rewards={pos_rewards} trades={trades}")
    
    if trades < 5:
        raise ValueError(f"Audit failed: trades={trades}<5 (info lost or filter too strict)")
    if pos_rewards < 1:
        raise ValueError(f"Audit failed: no positive rewards (wrong reward config)")
    
    print(f"[AUDIT] SUCCESS: Environment is working correctly")
    # ê°ì‚¬ í›„ í™˜ê²½ ë¦¬ì…‹
    ev.reset()
    return True


def _probe_eval_pipeline(main_config=None, df_ppo=None, tft_model=None, feature_pipeline=None, env_fn=None):
    """í‰ê°€ íŒŒì´í”„ë¼ì¸ ê²€ì¦ (ëª¨ë¸ ì—†ì´ ê°•ì œ ì•¡ì…˜ìœ¼ë¡œ)"""
    print("[Probe] Starting evaluation pipeline probe...")
    
    # ê°„ë‹¨í•œ í‰ê°€ìš© í™˜ê²½ ìƒì„±
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.vec_env import VecNormalize
    
    # ê¸°ë³¸ í™˜ê²½ ìƒì„± (offset=0)
    trial_seed = (main_config.get("seed", 42) if main_config else 42) + 999  # í…ŒìŠ¤íŠ¸ìš© ì‹œë“œ
    if df_ppo is not None and feature_pipeline is not None and tft_model is not None:
        env = make_env_with_offset(df_ppo, tft_model, feature_pipeline.features, offset=0)()
    elif env_fn is not None:
        env = env_fn()
    else:
        print("[Probe] WARNING: No environment factory available, skipping probe")
        return False
    
    # í‰ê°€ìš© íŒŒë¼ë¯¸í„° ì„¤ì •
    if hasattr(env, "reward_mode"):    env.reward_mode = "pnl_delta"
    if hasattr(env, "fee_rate"):       env.fee_rate = 3 / 1e4
    if hasattr(env, "slippage_rate"):  env.slippage_rate = 1 / 1e4
    if hasattr(env, "sanity_mode"):    env.sanity_mode = False
    
    from gymnasium.wrappers import TimeLimit
    eval_steps = (main_config.get("eval", {}).get("max_steps", 1000) if main_config else 1000)
    env = TimeLimit(env, max_episode_steps=int(eval_steps))
    env.reset(seed=trial_seed)
    
    # VecEnvë¡œ ë˜í•‘
    ev = DummyVecEnv([lambda: env])
    ev = VecNormalize(ev, norm_obs=True, norm_reward=False, clip_obs=5.0, clip_reward=float("inf"))
    ev.training = False
    ev.norm_reward = False
    
    obs = ev.reset()[0]
    trades = 0
    
    for i in range(30):
        a = np.array([1 if i%2==0 else 2], dtype=np.int64)  # 1,2 ë²ˆê°ˆì•„
        obs, reward, dones, infos = ev.step(a)
        t = _extract_trade(infos)
        info0 = infos[0] if isinstance(infos, (list, tuple)) and len(infos)>0 else infos
        print(f"[Probe] i={i} forced_action={a[0]} trade={t} info_keys={list(info0.keys()) if isinstance(info0, dict) else type(info0)}")
        trades += t
        if bool(dones[0]): 
            break
    
    ev.close()
    print(f"[Probe] total_trades={trades}")
    
    if trades > 0:
        print("[Probe] SUCCESS: Pipeline is working, trades detected")
    else:
        print("[Probe] FAILED: No trades detected in pipeline")
    
    return trades > 0


def max_drawdown_equity(eq):
    """ì—ì¿¼í‹° ì‹œê³„ì—´ì—ì„œ ìµœëŒ€ ë“œë¡œìš°ë‹¤ìš´ ê³„ì‚°"""
    eq = np.asarray(eq, dtype=float)
    peak = np.maximum.accumulate(eq)
    dd = (peak - eq) / np.maximum(peak, 1e-9)
    return float(np.max(dd)) if dd.size else 0.0


import gymnasium as gym
import numpy as np

class NoOpStreakPenaltyWrapper(gym.Wrapper):
    def __init__(self, env, patience=5, penalty=1e-4, max_penalty=5e-4):
        super().__init__(env)
        self.patience = patience
        self.penalty = penalty
        self.max_penalty = max_penalty
        self._streak = 0

    def reset(self, **kwargs):
        self._streak = 0
        return self.env.reset(**kwargs)

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        # SB3 Discrete: 0=hold ê°€ì •
        a = int(action[0] if isinstance(action, (list, np.ndarray)) else action)
        if a == 0:
            self._streak += 1
            if self._streak >= self.patience:
                # í˜ë„í‹°ëŠ” ëˆ„ì§„(ìƒí•œ ìº¡)
                p = min(self.penalty * (self._streak - self.patience + 1), self.max_penalty)
                reward = float(reward) - p
                info["noop_penalty"] = p
        else:
            self._streak = 0
        return obs, reward, terminated, truncated, info


# === PnL ê¸°ë°˜ í‰ê°€ í—¬í¼ ===
def _extract_pnl(infos):
    # VecEnvì¼ ìˆ˜ ìˆì–´ list/tuple ë°©ì–´
    if isinstance(infos, (list, tuple)):
        for d in infos:
            if isinstance(d, dict):
                if "pnl_step" in d: return float(d["pnl_step"])
                if "pnl" in d:      return float(d["pnl"])  # ë°±ì—…
        return 0.0
    if isinstance(infos, dict):
        return float(infos.get("pnl_step", infos.get("pnl", 0.0)))
    return 0.0

def _equity_from_pnl_series(pnl_series, start_equity=1.0):
    # pnl_stepì€ ë¡œê·¸ìˆ˜ìµ ê°€ì • â†’ equity = exp(cumsum)
    pnl = np.asarray(pnl_series, dtype=np.float64)
    return start_equity * np.exp(np.cumsum(pnl))

def _sharpe_from_pnl(pnl_series, steps_per_year=24*365, eps=1e-12):
    r = np.asarray(pnl_series, dtype=np.float64)
    mu, sd = float(np.mean(r)), float(np.std(r))
    if sd < eps: 
        return 0.0
    return float((mu / sd) * np.sqrt(steps_per_year))

# === ìŠ¤ë§ˆíŠ¸ ê±°ë˜ ì¹´ìš´í„° í—¬í¼ ===
def _trade_counter_begin():
    """ê±°ë˜ ì¹´ìš´í„° ìƒíƒœ ë¦¬ì…‹"""
    _extract_trade.__defaults__ = (False, {"last": 0})

def _trade_count_smart(infos):
    """
    trade í”Œë˜ê·¸/ì¹´ìš´í„° ìë™ íŒë³„:
    - ìš°ì„  ë¸íƒ€ ì‹œë„(ì¹´ìš´í„° ê°€ì •)
    - ë¸íƒ€ê°€ 0ì´ê³  í”Œë˜ê·¸ê°€ 1ì´ë©´(=ì¹´ìš´í„° ì•„ë‹˜) í”Œë˜ê·¸ë¥¼ í•©ì‚°
    """
    # raw flag
    raw = _extract_trade(infos, as_delta=False)
    # delta (counter ê°€ì •)
    delta = _extract_trade(infos, as_delta=True)
    if delta > 0:
        return delta
    # delta=0ì¸ë° raw=1 ì´ë©´ per-step flagë¡œ ê°„ì£¼
    return 1 if raw == 1 else 0
def _score_from_kpis(sharpe, mdd, trades):
    """ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ - í˜„ì‹¤ì ì¸ ë²”ìœ„ë¡œ ìˆ˜ì •"""
    if trades == 0:   return -2.0
    if trades < 3:    return -1.6
    if trades < 5:    return -1.3

    # ìƒ¤í”„â†‘(0.6), MDDâ†“(-1.5), ê±°ë˜ ì†ŒëŸ‰ ë³´ë„ˆìŠ¤(+0.1)
    score = 0.6*sharpe - 1.5*mdd + 0.1*min(trades, 50)/50.0
    # ê³¼ë§¤ë§¤(ìŠ¤í… ëŒ€ë¹„ ê±°ë˜ ë¹„ì¤‘) ì™„ë§Œ íŒ¨ë„í‹°ë¥¼ ì›í•˜ë©´ ì—¬ê¸°ì„œ ì¡°ì •
    return float(np.clip(score, -2.0, 3.0))


# =========================
# í‰ê°€ ìœ í‹¸ (ì†ë„ëª¨ë“œ í¬í•¨)
# =========================
def _evaluate_vectorized(model, make_env_fn, episodes: int = 3, fee_bps: float = 10.0, ann_factor: float = 365.0):
    """
    ê°„ë‹¨í•œ í™˜ê²½ ë³´ìƒ ê¸°ë°˜ í‰ê°€ (í•„í„° ì ìš©ëœ í™˜ê²½ ì‚¬ìš©)
    """
    from gymnasium.wrappers import TimeLimit

    scores = []

    for episode in range(episodes):
        env = make_env_fn(offset=episode * 500)  # ì„œë¡œ ë‹¤ë¥¸ ì‹œì‘ì 
        eval_max_steps = 1000
        if not isinstance(env, TimeLimit):
            env = TimeLimit(env, max_episode_steps=eval_max_steps)

        obs = env.reset()[0]
        rewards = []
        trade_count = 0
        done = False
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = bool(terminated or truncated)
            rewards.append(float(reward))
            
            # âœ… ì‹¤ì œ ê±°ë˜ ì¹´ìš´íŠ¸
            if isinstance(info, dict):
                trade_count += int(info.get("trade", 0))

        env.close()
        
        # ë³´ìƒ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        if len(rewards) < 10:
            scores.append(-1.0)
            continue
            
        score = _score_rewards(rewards, freq="1h", trades_override=trade_count)
        scores.append(score)

    score = float(np.mean(scores))
    print(f"[EvalDebug] Evaluation score: {score:.6f}")
    return score


def _score_rewards(rewards, freq="1h", trades_override=None):
    """ë³´ìƒ ì‹œí€€ìŠ¤ë¥¼ KPI ì ìˆ˜ë¡œ ë³€í™˜ - ìƒì„¸ ë¡œê·¸ í¬í•¨(ìŠ¤í”¼ë“œëª¨ë“œì—ì„œ ì‚¬ìš©)"""
    if not rewards or len(rewards) < 10:
        print(f"[EvalDebug] Too short rewards: len={len(rewards) if rewards else 0} -> penalty -1.0")
        return -1.0

    rew = np.array(rewards, dtype=float)
    mu = float(np.mean(rew))
    sd = float(np.std(rew, ddof=1))
    if sd == 0 or not np.isfinite(sd):
        sharpe = -1.0
    else:
        ann = 24*365 if freq in ["1h", "h", "hour", "hourly"] else 365
        sharpe = (mu / sd) * math.sqrt(ann)

    winrate = float(np.mean(rew > 0))
    equity = np.exp(np.cumsum(rew))
    peak = np.maximum.accumulate(equity)
    dd = float(np.max((peak - equity) / np.maximum(peak, 1e-12)))

    # âœ… ê±°ë˜ íšŸìˆ˜ëŠ” overrideê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
    if trades_override is not None:
        trades = int(trades_override)
    else:
        trades = 0  # ì•ˆì „ ê¸°ë³¸ê°’

    # ê³¼ë§¤ë§¤ í˜ë„í‹° ê¸°ì¤€ì„ "ìŠ¤í… ëŒ€ë¹„ ë¹„ìœ¨"ë¡œ ë³€ê²½
    steps = len(rew)
    overtrading_ratio = trades / max(1, steps)
    penalty_reason = "none"

    if trades == 0:
        score = -2.0
        penalty_reason = "trades=0"
    elif trades < 5:
        score = -1.5 + (trades / 5.0) * 0.5
        penalty_reason = f"trades={trades}<5"
    elif overtrading_ratio > 0.5:              # ìŠ¤í…ì˜ 50% ì´ˆê³¼ ê±°ë˜ë©´ ê³¼ë§¤ë§¤
        score = -1.0
        penalty_reason = f"overtrading={trades}"
    else:
        score = (
            0.6 * np.clip(sharpe, -2.0, 5.0)
            + 0.3 * (winrate - 0.5) * 2.0
            - 0.1 * np.clip(dd, 0.0, 0.5) * 5
            + 0.1 * np.clip(trades / 50.0, 0.0, 1.0)
        )
        penalty_reason = "none"

    print(f"[EvalDebug] len={len(rew)} | trades={trades} | sharpe={sharpe:.3f} | winrate={winrate:.3f} | mdd={dd:.3f} | penalty={penalty_reason} -> score={score:.3f}")
    print(f"[EvalDebug] reward_stats: mean={mu:.6f} | std={sd:.6f} | min={rew.min():.6f} | max={rew.max():.6f}")
    return float(score)


def _evaluate(model: PPO, make_env_fn, episodes: int, freq: str) -> float:
    ann_factor = 24 * 365 if freq in ["1h", "h", "hour", "hourly"] else 365
    score = _evaluate_vectorized(model, make_env_fn=make_env_fn, episodes=episodes, fee_bps=10.0, ann_factor=ann_factor)
    return score


# =========================
# ì•ˆì •í™” ë˜í¼
# =========================
class StickyActionWrapper(gym.Wrapper):
    """ì´ˆê¸° íƒìƒ‰ ì•ˆì •í™”ë¥¼ ìœ„í•œ sticky action"""
    def __init__(self, env, prob: float = 0.25):
        super().__init__(env)
        self.prob = prob
        self._last = None

    def step(self, action):
        import random

        if self._last is not None and random.random() < self.prob:
            action = self._last
        self._last = action
        return self.env.step(action)


# =========================
# Optuna main
# =========================
def tune_ppo(
    env_fn: Callable[[], Any],
    main_config: Dict[str, Any],
    optuna_cfg_path: str = "user_data/TFT_PPO_Training/configs/optuna_config.yml",
    tft_model=None,
    df_ppo=None,
    feature_pipeline=None,
) -> Dict[str, Any]:
    with open(optuna_cfg_path, "r") as f:
        ocfg = yaml.safe_load(f)["optuna"]

    sampler = _build_sampler(ocfg.get("sampler", {}))
    pruner = _build_pruner(ocfg.get("pruner", {}))
    study = optuna.create_study(
        study_name=ocfg.get("study_name", "ppo_tuning"),
        direction=ocfg.get("direction", "maximize"),
        storage=ocfg.get("storage", None),
        load_if_exists=True,
        sampler=sampler,
        pruner=pruner,
    )

    # ì „ì—­/ê¸°ë³¸ í‰ê°€ ì„¤ì •
    n_trials = int(ocfg.get("n_trials", 20))
    timeout = ocfg.get("timeout", None)
    n_jobs = int(ocfg.get("n_jobs", 1))
    data_freq = main_config.get("timeframe", "1h")

    def objective(trial: optuna.trial.Trial) -> float:
        # âœ… í•™ìŠµì€ ë¹„ê²°ì • (í‰ê°€ë§Œ ê²°ì •)
        set_seed(main_config.get("seed", 42) + trial.number, deterministic=False)
        
        # ë””ë²„ê·¸: íŒŒì¼ ê²½ë¡œ/ì‹œê°„ í™•ì¸
        import os, time
        print(f"[DEBUG] using optuna_tuning.py at {__file__} mtime={time.ctime(os.path.getmtime(__file__))}")

        # ì •ì±… ë„¤íŠ¸ì›Œí¬ ê°•ì œ ì¬ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜
        def _force_policy_reinit(model):
            """ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ ì™„ì „íˆ ì¬ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
            import torch
            import torch.nn as nn
            
            # ì •ì±… ë„¤íŠ¸ì›Œí¬ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ì¬ì´ˆê¸°í™”
            for module in model.policy.modules():
                if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                    nn.init.xavier_uniform_(module.weight)
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
                elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):
                    nn.init.ones_(module.weight)
                    nn.init.zeros_(module.bias)
            
            print("[PolicyReinit] Policy network weights completely reinitialized")
            return model

        # 1) íƒìƒ‰ê³µê°„ & ì•ˆì „ í´ë¨í”„
        params = _suggest_params(trial, ocfg["search_space"])
        # íƒìƒ‰ ì•ˆì •í™”ë¥¼ ìœ„í•´ ìƒí•œ ì¡°ì •
        params["ent_coef"] = float(np.clip(params.get("ent_coef", 0.0), 0.002, 0.03))  # ì •ìƒ ë²”ìœ„ë¡œ ë³µì›
        params["clip_range"] = float(np.clip(params.get("clip_range", 0.2), 0.10, 0.25))
        params["gamma"] = float(np.clip(params.get("gamma", 0.99), 0.98, 0.992))  # ê°ë§ˆ ë²”ìœ„ í˜„ì‹¤ì ìœ¼ë¡œ ì¡°ì •
        params["batch_size"] = int(params.get("batch_size", 256))
        print(f"[PARAM-OVERRIDE] ent_coef={params['ent_coef']:.4f} clip_range={params['clip_range']:.2f} gamma={params['gamma']:.3f} batch={params['batch_size']}")

        print("[Optuna] TFT embedding cache disabled for trial diversity")

        # 2) PPO kwargs (ìŠ¤ì¼€ì¤„ì€ í•™ìŠµ ì¤‘ì— ë™ì ìœ¼ë¡œ ì¡°ì •)
        ppo_kwargs = dict(
            learning_rate=params["learning_rate"],
            gamma=params["gamma"],
            clip_range=params["clip_range"],
            ent_coef=0.07,  # ğŸ”¼ ì´ˆê¸°ê°’ì„ ë†’ê²Œ ì„¤ì • (í•™ìŠµ ì¤‘ ë™ì  ì¡°ì •)
            batch_size=params["batch_size"],
            vf_coef=params.get("vf_coef", 0.5),
            max_grad_norm=params.get("max_grad_norm", 1.0),
            device="cpu",
            verbose=0,
            seed=main_config.get("seed", 42),
            n_steps=main_config["ppo"].get("n_steps", 2048),
            target_kl=0.015,                         # ğŸ”’ ì ë¦¼ ê°€ì† ì—…ë°ì´íŠ¸ ì»·
        )

        # 3) í•™ìŠµìš© VecNormalize + StickyAction + MinHoldCooldown ì ìš©
        def make_trial_env():
            trial_seed = main_config.get("seed", 42) + trial.number
            env = env_fn(trial_seed=trial_seed)  # ì‚¬ìš©ìê°€ ë„˜ê¸°ëŠ” TradingEnv factory
            
            # í™˜ê²½ ì„¤ì • ê²€ì¦ ë° ë¡œê¹… (ë˜í¼ ì „ì—)
            if hasattr(env, 'reward_mode'):
                print(f"[ENV-PARAMS] reward_mode={env.reward_mode}")
                assert env.reward_mode == "pnl_delta", f"Wrong reward_mode: {env.reward_mode}"
            
            if hasattr(env, 'fee_rate'):
                print(f"[ENV-PARAMS] fee_rate={env.fee_rate}")
                assert abs(env.fee_rate - 0.00015) < 1e-6, f"Wrong fee_rate: {env.fee_rate}"
            
            # âš™ï¸ í™˜ê²½ ìƒì„± ì‹œ ì˜µì…˜ ê°•ì œ í†µì¼: ë³´ìƒ/ë¹„ìš©/ëª¨ë“œ
            env.reward_mode = "pnl_delta"
            env.fee_rate = 3 / 1e4  # fee_bps=3
            env.slippage_rate = 1 / 1e4  # slippage_bps=1
            if hasattr(env, "sanity_mode"):
                env.sanity_mode = False  # í•™ìŠµì€ Falseë¡œ
            
            # TimeLimit ë˜í•‘ (ë˜í¼ ì „ì—)
            from gymnasium.wrappers import TimeLimit
            eval_steps = main_config.get("eval", {}).get("max_steps", 1000)
            env = TimeLimit(env, max_episode_steps=int(eval_steps))
            
            # Sticky/ActionFilterëŠ” í‰ê°€ì—ì„œ ë¹„í™œì„±í™” (ì •ëŸ‰ ë¹„êµ ëª©ì )
            # í›ˆë ¨ì—ì„œëŠ” íƒìƒ‰ì„ ìœ„í•´ ë” ê³µê²©ì ìœ¼ë¡œ í™œì„±í™”
            env = StickyActionWrapper(env, prob=0.15)  # 0.4 â†’ 0.15ë¡œ ì™„í™”
            
            # MinHoldCooldownì€ ìœ ì§€ (ê³¼ë§¤ë§¤ ë°©ì§€)
            from TFT_PPO_Training.scripts.wrappers import MinHoldCooldownWrapper
            env = MinHoldCooldownWrapper(env, min_hold=3, cooldown=2)
            
            # ìƒˆë¡œ ì¶”ê°€: ì•ˆì •í™”ëœ íƒìƒ‰ ê°•í™” ë˜í¼ë“¤ (í›ˆë ¨ ì „ìš©)
            from TFT_PPO_Training.scripts.wrappers import EpsGreedyWrapper, SameActionPenaltyWrapper
            # ActionCycleWrapperëŠ” ì ì • ë¹„í™œì„±í™” (í•™ìŠµ ì‹ í˜¸ ì™œê³¡ í¼)
            # env = ActionCycleWrapper(env, cycle_length=100)
            
            # StickyAction ì œê±° (ì ë¦¼ ìœ ì§€ê¸° ì—­í• )
            # env = StickyActionWrapper(env, prob=0.15)  # âŒ ì œê±°
            
            # ìˆœì„œ: (íƒìƒ‰) â†’ (ë°˜ë³µë²Œ) â†’ (ëŒ€ê¸°ì—°ì†ë²Œ)
            env = EpsGreedyWrapper(env, eps=0.20)  # 0.15 â†’ 0.20 (ì´ˆê¸°ë§Œ ì‚´ì§ ì˜¬ë ¤ íƒìƒ‰ í™•ë³´)
            env = SameActionPenaltyWrapper(env, penalty=1e-4)  # 3e-5 â†’ 1e-4 (ë°˜ë³µ ì–µì œ ê°•í™”)
            env = NoOpStreakPenaltyWrapper(env, patience=5, penalty=1e-4, max_penalty=5e-4)  # ëŒ€ê¸°ì—°ì†ë²Œ
            
            if hasattr(env, "reset"):
                env.reset(seed=trial_seed)
            return env

        def make_eval_env():
            trial_seed = main_config.get("seed", 42) + trial.number
            env = env_fn(trial_seed=trial_seed)  # ì‚¬ìš©ìê°€ ë„˜ê¸´ ìˆœìˆ˜ TradingEnv íŒ©í† ë¦¬
            # í‰ê°€ìš© ê³µí†µ íŒŒë¼ë¯¸í„° í†µì¼
            if hasattr(env, "reward_mode"):    env.reward_mode = "pnl_delta"
            if hasattr(env, "fee_rate"):       env.fee_rate = 3 / 1e4
            if hasattr(env, "slippage_rate"):  env.slippage_rate = 1 / 1e4
            if hasattr(env, "sanity_mode"):    env.sanity_mode = False
            
            # í‰ê°€ì—ì„œëŠ” ìˆœìˆ˜í•œ ì •ì±… ì„±ëŠ¥ ì¸¡ì • (Îµ-greedy ì œê±°)
            # from TFT_PPO_Training.scripts.wrappers import EpsGreedyWrapper
            # env = EpsGreedyWrapper(env, eps=0.05)  # í‰ê°€ìš©ìœ¼ë¡œ ë” ë‚®ì€ í™•ë¥ 
            
            from gymnasium.wrappers import TimeLimit
            eval_steps = main_config.get("eval", {}).get("max_steps", 1000)
            env = TimeLimit(env, max_episode_steps=int(eval_steps))
            env.reset(seed=trial_seed)
            return env

        venv = DummyVecEnv([make_trial_env])
        venv = VecNormalize(venv, norm_obs=True, norm_reward=False, clip_obs=5.0, clip_reward=float("inf"))

        model = PPO("MlpPolicy", venv, **ppo_kwargs)
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ ê°•ì œ ì¬ì´ˆê¸°í™” (ë§¤ Trialë§ˆë‹¤)
        model = _force_policy_reinit(model)
        
        if hasattr(model.policy, "reset_parameters"):
            model.policy.reset_parameters()
        print(f"[Optuna] Trial {trial.number} - PPO model initialized with fresh weights")

        total_ts = int(main_config.get("optuna", {}).get("timesteps", 200_000))

        # í‰ê°€ ì„¤ì • (speed mode ê¸°ë³¸)
        eval_config = main_config.get("eval", {})
        speed_mode = eval_config.get("speed_mode", True)
        eval_warmup_steps = max(100_000, eval_config.get("warmup_steps", 100_000))  # ìµœì†Œ 100k
        eval_every = eval_config.get("every", 50_000)
        eval_max_steps = eval_config.get("max_steps", 2000)   # â† 2000ìœ¼ë¡œ ì¦ê°€
        eval_episodes = eval_config.get("episodes", 2)        # â† 2ê°œ ì—í”¼ì†Œë“œë¡œ ì¦ê°€
        eval_offsets = eval_config.get("offsets", [0, 800, 1600])   # â† ì„œë¡œ ë‹¤ë¥¸ ì‹œì‘ì  3ê°œ

        # í‰ê°€ìš© VecNormalize venv ìƒì„±ì (í•™ìŠµ í†µê³„ ê³µìœ )
        def _make_eval_venv():
            e = DummyVecEnv([make_eval_env])  # â˜… ë˜í¼ ì—†ëŠ” í‰ê°€ìš©
            ev = VecNormalize(e, norm_obs=True, norm_reward=False, clip_obs=5.0, clip_reward=float("inf"))
            ev.obs_rms = venv.obs_rms
            ev.ret_rms = venv.ret_rms
            ev.training = False
            ev.norm_reward = False
            return ev

        def _evaluate_with_vecnorm(model, steps=1000) -> float:
            ev = _make_eval_venv()

            # ì²« í‰ê°€ ì‹œ ê°ì‚¬
            if not hasattr(_evaluate_with_vecnorm, '_audited'):
                _audit_env(ev)
                _probe_eval_pipeline(main_config, df_ppo, tft_model, feature_pipeline, env_fn)  # íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì¶”ê°€
                _evaluate_with_vecnorm._audited = True

            obs = ev.reset()[0]
            
            # ê±°ë˜ ì¹´ìš´íŠ¸ ìƒíƒœ ë¦¬ì…‹ (ìŠ¤ë§ˆíŠ¸ ì§‘ê³„ìš©)
            _trade_counter_begin()
            
            pnl_list, rewards, trade_count = [], [], 0
            done = False

            while not done:
                action, _ = model.predict(obs, deterministic=True)   # â˜… ê²°ì •ì 
                # ì•ˆì „ í¬ì¥: (1,) í˜•íƒœì˜ int64 ë³´ì¥
                if isinstance(action, np.ndarray):
                    if action.ndim == 0:
                        action = action.reshape(1,)
                    elif action.ndim > 1:
                        action = action.squeeze()
                else:
                    action = np.array([action])
                action = action.astype(np.int64)
                obs, reward, dones, infos = ev.step(action)
                done = bool(dones[0])
                rewards.append(float(reward[0]))                     # ë¡œê¹…ìš©
                pnl_list.append(_extract_pnl(infos))                 # â˜… í‰ê°€ í•µì‹¬
                trade_count += _extract_trade(infos)                 # í”Œë˜ê·¸ í•©ì‚° (as_delta=False)
                
                # ë””ë²„ê·¸ ë¡œê·¸ (200ìŠ¤í…ë§ˆë‹¤)
                if len(pnl_list) % 200 == 0:
                    print(f"[StepDbg] t={len(pnl_list)} trade+={_extract_trade(infos)} total_trades={trade_count}")

            ev.close()

            equity = _equity_from_pnl_series(pnl_list, start_equity=1.0)
            mdd = max_drawdown_equity(equity)
            sharpe = _sharpe_from_pnl(pnl_list, steps_per_year=24*365)
            winrate = float(np.mean(np.array(pnl_list) > 0.0))
            final = _score_from_kpis(sharpe, mdd, trade_count)

            print(f"[EvalKPIs] len={len(pnl_list)} trades={trade_count} sharpe={sharpe:.3f} winrate={winrate:.3f} mdd={mdd:.3f}")
            print(f"[EvalStats] reward: mean={np.mean(rewards):.6f} std={np.std(rewards):.6f} min={np.min(rewards):.6f} max={np.max(rewards):.6f}")
            print(f"[EvalStats] pnl   : mean={np.mean(pnl_list):.6f} std={np.std(pnl_list):.6f} min={np.min(pnl_list):.6f} max={np.max(pnl_list):.6f}")
            print(f"[EvalScore] final_score={final:.6f} (sharpe={sharpe:.3f}, mdd={mdd:.3f}, trades={trade_count})")
            return final

        learned = 0
        best_score = -np.inf
        print(f"[Optuna] Trial {trial.number} start | total_ts={total_ts} | speed_mode={speed_mode} | params={params}")

        while learned < total_ts:
            # Îµ-greedy ìŠ¤ì¼€ì¤„: ì´ˆë°˜ë§Œ ì„¸ê²Œ, ì´í›„ ê¸‰ê°
            if learned < 60_000:
                try: venv.envs[0].env.env.set_eps(0.30)  # 0.15 â†’ 0.30 (ì´ˆë°˜ë§Œ ê°•íƒìƒ‰)
                except: pass
            elif learned < 120_000:
                try: venv.envs[0].env.env.set_eps(0.10)
                except: pass
            else:
                try: venv.envs[0].env.env.set_eps(0.05)
                except: pass
            
            chunk = min(10_000, total_ts - learned)
            
            # ent_coef ë™ì  ì¡°ì • (ìŠ¤ì¼€ì¤„ ì ìš©)
            progress = learned / total_ts
            remaining = 1.0 - progress
            if remaining > 0.5:  # ì´ˆê¸° 50% êµ¬ê°„
                model.ent_coef = 0.07
            elif remaining > 0.2:  # ì¤‘ê¸° 30% êµ¬ê°„
                model.ent_coef = 0.04
            else:  # í›„ê¸° 20% êµ¬ê°„
                model.ent_coef = 0.012
            
            model.learn(total_timesteps=chunk, reset_num_timesteps=False, progress_bar=False)
            learned += chunk
            print(f"[Optuna] Trial {trial.number} learned {learned}/{total_ts} timesteps ({learned/total_ts*100:.1f}%)")

            # ì›Œãƒ¼ãƒ ì—…: 100k ì´ì „ì—” í‰ê°€ skip
            if learned < eval_warmup_steps:
                print(f"[Optuna] Trial {trial.number} warmup phase - skipping evaluation")
                continue
            if learned % eval_every != 0:
                continue

            # í‰ê°€ ì‹¤í–‰ (ì—¬ëŸ¬ ì˜¤í”„ì…‹ìœ¼ë¡œ í‰ê· )
            if speed_mode:
                scores = []
                for offset in eval_offsets:
                    def make_offset_eval_env(offset):
                        trial_seed = main_config.get("seed", 42) + trial.number
                        if df_ppo is not None and feature_pipeline is not None:
                            env = make_env_with_offset(df_ppo, tft_model, feature_pipeline.features, offset)()
                        else:
                            env = env_fn()
                        if hasattr(env, "reward_mode"):    env.reward_mode = "pnl_delta"
                        if hasattr(env, "fee_rate"):       env.fee_rate = 3 / 1e4
                        if hasattr(env, "slippage_rate"):  env.slippage_rate = 1 / 1e4
                        if hasattr(env, "sanity_mode"):    env.sanity_mode = False
                        
                        # í‰ê°€ì—ì„œëŠ” ìˆœìˆ˜í•œ ì •ì±… ì„±ëŠ¥ ì¸¡ì • (Îµ-greedy ì œê±°)
                        # from TFT_PPO_Training.scripts.wrappers import EpsGreedyWrapper
                        # env = EpsGreedyWrapper(env, eps=0.05)
                        
                        from gymnasium.wrappers import TimeLimit
                        eval_steps = main_config.get("eval", {}).get("max_steps", 1000)
                        env = TimeLimit(env, max_episode_steps=int(eval_steps))
                        env.reset(seed=trial_seed)
                        return env

                    def _make_offset_eval_venv(offset):
                        e = DummyVecEnv([lambda: make_offset_eval_env(offset)])
                        ev = VecNormalize(e, norm_obs=True, norm_reward=False, clip_obs=5.0, clip_reward=float("inf"))
                        ev.obs_rms = venv.obs_rms
                        ev.ret_rms = venv.ret_rms
                        ev.training = False
                        ev.norm_reward = False
                        return ev

                    def _evaluate_offset(model, offset, steps):
                        ev = _make_offset_eval_venv(offset)
                        obs = ev.reset()[0]
                        
                        # ê±°ë˜ ì¹´ìš´íŠ¸ ìƒíƒœ ë¦¬ì…‹ (ìŠ¤ë§ˆíŠ¸ ì§‘ê³„ìš©)
                        _trade_counter_begin()
                        
                        pnl_list, rewards, trade_count, done = [], [], 0, False
                        epsilon_probe_used = False
                        
                        # ë””ë²„ê¹… ë³€ìˆ˜ë“¤
                        step_i = 0
                        act_hist = {0:0, 1:0, 2:0}
                        trade_ones = 0
                        
                        while not done:
                            action, _ = model.predict(obs, deterministic=True)
                            
                            # ì •ì±… ë¶„í¬ ì§„ë‹¨ (200ìŠ¤í… ê°„ê²©)
                            if len(pnl_list) % 200 == 0:
                                try:
                                    dist = model.policy.get_distribution(obs)
                                    probs = getattr(dist.distribution, "probs", None)
                                    if probs is not None:
                                        p = probs[0].detach().cpu().numpy()
                                        if len(p) == 3:
                                            print(f"[PiProbs] t={len(pnl_list)} p(a0,a1,a2)={p[0]:.3f},{p[1]:.3f},{p[2]:.3f}")
                                except:
                                    pass
                            
                            # --- Îµ-probe: ê²°ì •ì  ì •ì±…ì´ tradesë¥¼ ëª» ë§Œë“¤ ë•Œë§Œ ê°€ë” ì°”ëŸ¬ë´„ ---
                            if trade_count < 3 and np.random.rand() < 0.05:
                                # action ê³µê°„ì´ Discrete(3)ë¼ê³  ê°€ì •: 0,1,2 ì¤‘ì—ì„œ ë¬´ì‘ìœ„
                                action = np.array([np.random.randint(0, 3)], dtype=np.int64)
                                epsilon_probe_used = True
                            # -------------------------------------------------------------
                            
                            # ì•ˆì „ í¬ì¥: (1,) í˜•íƒœì˜ int64 ë³´ì¥
                            if isinstance(action, np.ndarray):
                                if action.ndim == 0:
                                    action = action.reshape(1,)
                                elif action.ndim > 1:
                                    action = action.squeeze()
                            else:
                                action = np.array([action])
                            action = action.astype(np.int64)
                            
                            obs, reward, dones, infos = ev.step(action)
                            done = bool(dones[0])
                            
                            # ë””ë²„ê¹… ì •ë³´ ìˆ˜ì§‘ (ì‹¤ì œ í™˜ê²½ì— ì „ë‹¬ëœ ì•¡ì…˜ ì¹´ìš´íŠ¸)
                            # actionì€ ì´ë¯¸ í™˜ê²½ì— ì „ë‹¬ëœ í›„ì´ë¯€ë¡œ ì‹¤ì œ ì•¡ì…˜ì„
                            a0 = int(action[0])
                            act_hist[a0] = act_hist.get(a0, 0) + 1
                            
                            t = _extract_trade(infos)
                            if t > 0:
                                trade_ones += 1
                            
                            rewards.append(float(reward[0]))
                            pnl_list.append(_extract_pnl(infos))
                            trade_count += _trade_count_smart(infos)  # ìŠ¤ë§ˆíŠ¸ ì§‘ê³„
                            
                            step_i += 1
                        ev.close()
                        print(f"[ActionHist] {act_hist} | trade_ones_in_first={trade_ones}")
                        
                        equity = _equity_from_pnl_series(pnl_list, start_equity=1.0)
                        mdd = max_drawdown_equity(equity)
                        sharpe = _sharpe_from_pnl(pnl_list, steps_per_year=24*365)
                        score = _score_from_kpis(sharpe, mdd, trade_count)
                        
                        print(f"[OffsetEval] offset={offset} len={len(pnl_list)} trades={trade_count} sharpe={sharpe:.3f} mdd={mdd:.3f} score={score:.6f} probe_used={epsilon_probe_used}")
                        return score

                    score = _evaluate_offset(model, offset, steps=eval_max_steps)
                    scores.append(score)
                
                # ì—¬ëŸ¬ ì˜¤í”„ì…‹ì˜ í‰ê·  ì ìˆ˜ ì‚¬ìš©
                score = float(np.mean(scores))
                print(f"[MultiOffset] scores={[f'{s:.3f}' for s in scores]} -> avg_score={score:.6f}")
            else:
                # ë” ì •í™•í•œ í‰ê°€ê°€ í•„ìš”í•˜ë©´ ì•„ë˜ë¥¼ í™•ì¥
                score = _evaluate_with_vecnorm(model, steps=eval_max_steps * 2)

            trial.report(score, step=learned)
            print(f"[Optuna] Trial {trial.number} evaluation: score={score:.6f} at step={learned}")
            print(f"[Optuna] Trial {trial.number} params: lr={params.get('learning_rate', 0):.2e}, gamma={params.get('gamma', 0):.3f}, ent_coef={params.get('ent_coef', 0):.4f}")

            if score > best_score:
                best_score = score

            if trial.should_prune():
                venv.close()
                raise optuna.TrialPruned()

        venv.close()
        return best_score

    study.optimize(objective, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs, gc_after_trial=True)

    print("Best parameters found:")
    for k, v in study.best_params.items():
        print(f"  {k}: {v}")

    return study.best_params
