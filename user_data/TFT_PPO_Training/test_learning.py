#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- ëœë¤ ì •ì±…ìœ¼ë¡œ í™˜ê²½ í…ŒìŠ¤íŠ¸
- PPO ëª¨ë¸ì˜ ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import numpy as np

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

def test_random_policy():
    """ëœë¤ ì •ì±…ìœ¼ë¡œ í™˜ê²½ í…ŒìŠ¤íŠ¸"""
    print("=== ëœë¤ ì •ì±… í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from TFT_PPO_modules.feature_pipeline import FeaturePipeline
        from TFT_PPO_modules.trading_env import TradingEnv
        from TFT_PPO_Training.scripts.wrappers import PriceTapWrapper
        from gymnasium.wrappers import TimeLimit
        import pandas as pd
        
        # ë°ì´í„° ë¡œë”©
        df = pd.read_feather("user_data/data/binance/BTC_USDT-1h.feather")
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)
        
        # í”¼ì²˜ íŒŒì´í”„ë¼ì¸
        fp = FeaturePipeline()
        df = fp.add_features(df)
        
        # ìµœê·¼ 180ì¼ ë°ì´í„° ì‚¬ìš©
        cut_date = df["date"].max() - pd.Timedelta(days=180)
        df_subset = df[df["date"] >= cut_date].reset_index(drop=True)
        
        print(f"ë°ì´í„° ê¸¸ì´: {len(df_subset)}")
        
        # ë”ë¯¸ TFT ëª¨ë¸
        class DummyTFT:
            def eval(self):
                return self
            def __call__(self, x):
                batch_size = list(x.values())[0].shape[0] if x else 1
                return {
                    "prediction": np.random.randn(batch_size, 1),
                    "attention": np.random.randn(batch_size, 1, 1)
                }
        
        dummy_tft = DummyTFT()
        
        # í™˜ê²½ ìƒì„±
        env = TradingEnv(df_subset, dummy_tft, fp.features, reward_mode="pnl", fee_bps=10, slippage_bps=5)
        env = PriceTapWrapper(env)
        env = TimeLimit(env, max_episode_steps=1000)
        
        # ëœë¤ ì •ì±… ì‹¤í–‰
        obs, info = env.reset()
        actions = []
        rewards = []
        
        for step in range(100):  # 100 ìŠ¤í…ë§Œ í…ŒìŠ¤íŠ¸
            action = env.action_space.sample()  # ëœë¤ ì•¡ì…˜
            obs, reward, terminated, truncated, info = env.step(action)
            
            actions.append(action)
            rewards.append(reward)
            
            if terminated or truncated:
                break
        
        env.close()
        
        # ê²°ê³¼ ë¶„ì„
        actions = np.array(actions)
        action_counts = {0: np.sum(actions == 0), 1: np.sum(actions == 1), 2: np.sum(actions == 2)}
        
        print(f"ì´ ìŠ¤í…: {len(actions)}")
        print(f"ì•¡ì…˜ ë¶„í¬: Hold={action_counts[0]}, Buy={action_counts[1]}, Sell={action_counts[2]}")
        print(f"í‰ê·  ë³´ìƒ: {np.mean(rewards):.4f}")
        
        if action_counts[1] > 0:
            print("âœ… Buy ì•¡ì…˜ ë°œê²¬ - í™˜ê²½ì´ ì •ìƒ ì‘ë™")
            return True
        else:
            print("âŒ Buy ì•¡ì…˜ì´ ì—†ìŒ - í™˜ê²½ ë¬¸ì œ")
            return False
            
    except Exception as e:
        print(f"âŒ ëœë¤ ì •ì±… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ppo_basic():
    """PPO ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸"""
    print("\n=== PPO ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from stable_baselines3 import PPO
        from stable_baselines3.common.vec_env import DummyVecEnv
        from TFT_PPO_modules.feature_pipeline import FeaturePipeline
        from TFT_PPO_modules.trading_env import TradingEnv
        from TFT_PPO_Training.scripts.wrappers import PriceTapWrapper
        from gymnasium.wrappers import TimeLimit
        import pandas as pd
        
        # ë°ì´í„° ì¤€ë¹„
        df = pd.read_feather("user_data/data/binance/BTC_USDT-1h.feather")
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)
        
        fp = FeaturePipeline()
        df = fp.add_features(df)
        
        cut_date = df["date"].max() - pd.Timedelta(days=180)
        df_subset = df[df["date"] >= cut_date].reset_index(drop=True)
        
        # ë”ë¯¸ TFT
        class DummyTFT:
            def eval(self):
                return self
            def __call__(self, x):
                batch_size = list(x.values())[0].shape[0] if x else 1
                return {
                    "prediction": np.random.randn(batch_size, 1),
                    "attention": np.random.randn(batch_size, 1, 1)
                }
        
        dummy_tft = DummyTFT()
        
        # í™˜ê²½ ìƒì„± í•¨ìˆ˜
        def make_env():
            env = TradingEnv(df_subset, dummy_tft, fp.features, reward_mode="pnl", fee_bps=10, slippage_bps=5)
            env = PriceTapWrapper(env)
            env = TimeLimit(env, max_episode_steps=1000)
            return env
        
        # PPO ëª¨ë¸ ìƒì„± (ë†’ì€ ì—”íŠ¸ë¡œí”¼ë¡œ íƒìƒ‰ ê°•í™”)
        env = DummyVecEnv([make_env])
        model = PPO(
            "MlpPolicy", 
            env, 
            learning_rate=0.001,  # ë†’ì€ í•™ìŠµë¥ 
            ent_coef=0.1,         # ë†’ì€ ì—”íŠ¸ë¡œí”¼
            verbose=1,
            device="cpu"
        )
        
        print("PPO ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        
        # ì§§ì€ í•™ìŠµ
        print("í•™ìŠµ ì‹œì‘...")
        model.learn(total_timesteps=10000, progress_bar=True)
        
        # í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
        obs = env.reset()
        actions = []
        
        for step in range(50):  # 50 ìŠ¤í… í…ŒìŠ¤íŠ¸
            action, _ = model.predict(obs, deterministic=False)  # ë¹„ê²°ì •ì  ì˜ˆì¸¡
            obs, reward, done, info = env.step(action)
            
            actions.append(action[0])  # ë²¡í„°í™”ëœ í™˜ê²½ì´ë¯€ë¡œ [0] ì¸ë±ì‹±
            
            if done[0]:
                break
        
        env.close()
        
        # ê²°ê³¼ ë¶„ì„
        actions = np.array(actions)
        action_counts = {0: np.sum(actions == 0), 1: np.sum(actions == 1), 2: np.sum(actions == 2)}
        
        print(f"í•™ìŠµëœ ëª¨ë¸ ì•¡ì…˜ ë¶„í¬: Hold={action_counts[0]}, Buy={action_counts[1]}, Sell={action_counts[2]}")
        
        if action_counts[1] > 0:
            print("âœ… í•™ìŠµëœ ëª¨ë¸ì´ Buy ì•¡ì…˜ì„ ì„ íƒí•¨")
            return True
        else:
            print("âŒ í•™ìŠµëœ ëª¨ë¸ì´ Buy ì•¡ì…˜ì„ ì„ íƒí•˜ì§€ ì•ŠìŒ")
            return False
            
    except Exception as e:
        print(f"âŒ PPO í•™ìŠµ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ” í•™ìŠµ ë¬¸ì œ ì§„ë‹¨ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # 1. ëœë¤ ì •ì±… í…ŒìŠ¤íŠ¸
    random_ok = test_random_policy()
    
    if not random_ok:
        print("\nâŒ í™˜ê²½ ìì²´ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        return
    
    # 2. PPO ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸
    ppo_ok = test_ppo_basic()
    
    if not ppo_ok:
        print("\nâŒ PPO ëª¨ë¸ í•™ìŠµì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í•´ê²° ë°©ì•ˆ:")
        print("  1. í•™ìŠµë¥ ì„ ë” ë†’ì´ê¸°")
        print("  2. ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ë¥¼ ë” ë†’ì´ê¸°")
        print("  3. ì•¡ì…˜ í•„í„°ë¥¼ ë¹„í™œì„±í™”í•˜ê¸°")
        return
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ - í•™ìŠµ í™˜ê²½ì´ ì •ìƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
